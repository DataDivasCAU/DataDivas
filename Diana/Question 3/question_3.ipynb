{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c6db49-8d6a-4f8d-b0c5-bbb409c38e6d",
   "metadata": {},
   "source": [
    "## In what ways has social media contributed to the growth and visibility of the Fridays for Future climate movement?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af3bb7",
   "metadata": {},
   "source": [
    "This part focuses on setting up necessary functions to gather + process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fb4f1-9c80-41fc-9423-13bce65bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import requests\n",
    "from googleapiclient.discovery import build\n",
    "import keybert\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bbf14-7ae2-49e7-b061-6ae386674ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyA1i1_PzGcufIqeey4dqug3KtrofVBQfYs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey= api_key)\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    '''Function that flattens one list\n",
    "       i: list in list\n",
    "       o: list'''\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "# Created with the help of LLM\n",
    "def search_video(before=None, after=None, filename=\"input.csv\"):\n",
    "    '''Searches youtube videos with given parameters\n",
    "       and saves them as a csv\n",
    "       i: date as string before, date as string after, string filename\n",
    "       o: ids of videos, list of titles + comments + descriptions '''\n",
    "    all_videos = []\n",
    "    next_page_token = None\n",
    "    max_pages = 2\n",
    "    current_page = 0\n",
    "\n",
    "    # Iterates through max_pages pages and returns the videos\n",
    "    while current_page < max_pages:\n",
    "        parameters = {\n",
    "            \"q\": \"Friday for Future\",\n",
    "            \"maxResults\": 50,\n",
    "            \"part\": \"snippet\",\n",
    "            \"order\": \"relevance\",\n",
    "            \"type\": \"video\",\n",
    "            \"relevanceLanguage\":\"de\",\n",
    "            \"videoCategoryId\": \"25\" # Politics and news\n",
    "        }\n",
    "        if before is not None:\n",
    "            parameters[\"publishedBefore\"] = before\n",
    "        if after is not None:\n",
    "            parameters[\"publishedAfter\"] = after\n",
    "        if next_page_token:\n",
    "            parameters[\"pageToken\"] = next_page_token\n",
    "        \n",
    "\n",
    "        # Searches for videos with parameters and returns a JSON response\n",
    "        request = youtube.search().list(**parameters)\n",
    "        response = request.execute()\n",
    "        all_videos.extend(response[\"items\"])\n",
    "\n",
    "        # Switches to search in next page\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        current_page += 1\n",
    "\n",
    "    # List of video ids\n",
    "    ids = []\n",
    "    for item in all_videos:\n",
    "        ids.append(item[\"id\"][\"videoId\"])\n",
    "\n",
    "    # List of titles\n",
    "    titles = []\n",
    "    for item in all_videos:\n",
    "        titles.append(item[\"snippet\"][\"title\"])\n",
    "\n",
    "\n",
    "    # List of comments for each video\n",
    "    all_comments = []\n",
    "\n",
    "    # Loops through each videoid and gets the comments\n",
    "    for videoid in ids:\n",
    "        video_comments = []\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part = \"snippet\",\n",
    "                videoId = videoid,\n",
    "                maxResults = 10,\n",
    "                textFormat = \"plainText\",\n",
    "                order = \"relevance\"\n",
    "            )\n",
    "\n",
    "            # Returns JSON with comments\n",
    "            response = request.execute()\n",
    "            for c in response.get(\"items\", []):\n",
    "                # Iterates through each response and gets the textDisplay field\n",
    "                video_comments.append(c[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"])\n",
    "\n",
    "        # Comments may be disabled\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        all_comments.append(video_comments)\n",
    "    \n",
    "    # List of description for each video\n",
    "    descriptions = []\n",
    "\n",
    "    # Loops through each videoid and gets the descriptions\n",
    "    for video_id in ids:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part = \"snippet\",\n",
    "                id=video_id\n",
    "        )\n",
    "            \n",
    "            # Returns JSON with description\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                # Grabs description, returns \"\" if empty\n",
    "                description = item[\"snippet\"].get(\"description\", \"\")\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "             continue\n",
    "        \n",
    "    # Comments are list in list\n",
    "    comments = flatten(all_comments)\n",
    "\n",
    "    final_data = titles + comments + descriptions\n",
    "\n",
    "    # Turns data into a dataframe and saves as csv\n",
    "    df = pd.DataFrame(final_data, columns=[\"text\"])\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "    return ids, final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86544b-3807-4223-a4fe-4ecead52e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_count(video_list, counts=\"numbers.csv\", sorted_count=\"sorted.csv\"):\n",
    "    '''Counts the amount of videos per each day \n",
    "       and sorts videos by view count\n",
    "       i: dict with title, views, likes, comments and upload time\n",
    "       o: csv with amount of videos per day, \n",
    "          csv with videos sorted by view count'''\n",
    "    \n",
    "    df = pd.DataFrame(video_list)\n",
    "\n",
    "    # Ensures upload time is proper year_month_day date\n",
    "    df[\"upload time\"] = pd.to_datetime(df[\"upload time\"])\n",
    "    df[\"year_month_day\"] = df[\"upload time\"].dt.to_period(\"D\")\n",
    "\n",
    "    # Count the number of videos per day and sort by date\n",
    "    video_counts = df[\"year_month_day\"].value_counts().sort_index()\n",
    "\n",
    "    # Converts it into a new dataframe with two columns\n",
    "    df_counts = video_counts.reset_index()\n",
    "    df_counts.columns = [\"year_month_day\", \"video_count\"]\n",
    "    df_counts.to_csv(counts, encoding=\"utf-8\", index=False)\n",
    "\n",
    "    # Sorts the videos by view count\n",
    "    sorted_videos = sorted(video_list, key=lambda x: x[\"views\"], reverse=True)\n",
    "    df_sorted = pd.DataFrame(sorted_videos)\n",
    "    df_sorted.to_csv(sorted_count, encoding=\"utf-8\", index=False)\n",
    "    \n",
    "    return df_counts, df_sorted\n",
    "\n",
    "\n",
    "    \n",
    "def get_data(data_ids):\n",
    "    '''Gets title, views, likes, comments and upload time\n",
    "       for each video\n",
    "       i: list of video ids\n",
    "       o: dict with title, views, likes, comments and upload time'''\n",
    "    \n",
    "    video_data = []\n",
    "\n",
    "    for item in data_ids:\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part = \"snippet, statistics\",\n",
    "            id = item\n",
    "        )\n",
    "        response = request.execute()\n",
    "    \n",
    "        for video in response.get(\"items\", []):\n",
    "\n",
    "            # Extracts the title and upload time from each video snippet\n",
    "            title = video[\"snippet\"].get(\"title\", \"\")\n",
    "            upload_time = video[\"snippet\"].get(\"publishedAt\", \"\")\n",
    "\n",
    "            # Extracts the statistics for each video\n",
    "            stats = video.get(\"statistics\", {})\n",
    "            views = int(stats.get(\"viewCount\", 0))\n",
    "            likes = int(stats.get(\"likeCount\", 0))\n",
    "            comments = int(stats.get(\"commentCount\", 0))\n",
    "\n",
    "\n",
    "            # Dict with the video info\n",
    "            video_data.append({\n",
    "                \"title\": title,\n",
    "                \"views\": views,\n",
    "                \"likes\": likes,\n",
    "                \"comments\": comments,\n",
    "                \"upload time\": upload_time\n",
    "            \n",
    "        })\n",
    "    return video_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fca674-ab69-4073-b0cb-4f2e69bd3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98264a-5cb2-46ba-9987-6c376cb51f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts default german stopwords from spaCy (LLM)\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "german_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# Function from the exercise, changed to match our requirements\n",
    "def keywords(data):\n",
    "    '''Uses the LLM keybert to extract keywords of our video data\n",
    "    i: list of titles + comments + descriptions\n",
    "    '''\n",
    "    model = keybert.KeyBERT() \n",
    "    keywords_list = []\n",
    "\n",
    "    for text in data[\"text\"].dropna().astype(str):\n",
    "        # Extracts 1-word keyphrases from text \n",
    "        kw_single = model.extract_keywords(text, keyphrase_ngram_range=(1,1))\n",
    "        keywords_list.extend(kw_single)\n",
    "\n",
    "    # List of specific words and german stopwords that should be filtered out\n",
    "    ignore_words = [\"fridays\", \"fridays future\", \"video\", \"fuÃŸ\", \"sehen\", \"20\", \"quot\", \"mal\", \"herr\", \"innen\", \"echt\", \"19\" ]\n",
    "    all_stopwords = german_stopwords.union(ignore_words)\n",
    "\n",
    "    # Flattens the keywords and filters out ignored words\n",
    "    flattened_keywords = [kw for kw, score in keywords_list if kw.lower() not in all_stopwords]\n",
    "\n",
    "\n",
    "    # Count how many times each keyword appears and only keeps them if >2 times\n",
    "    keywords_df = pd.DataFrame(Counter(flattened_keywords).items(), columns=[\"words\", \"numbers\"])\n",
    "    frequent_words = keywords_df.loc[keywords_df[\"numbers\"] > 2].sort_values(by=\"numbers\", ascending=False)\n",
    "\n",
    "    frequent_words.to_csv(\"keywords.csv\", index=False, encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55707926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests the info from the school climate strikes wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_school_climate_strikes\"\n",
    "headers = {\"User-Agent\": \"Data Science Analytics (Dianer)\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Reads all HTML tables from the response and takes the first one\n",
    "tables = pd.read_html(StringIO(response.text))\n",
    "df = tables[0]\n",
    "\n",
    "# Filters for Strikes in Germany (or multiple countries, where germany is included)\n",
    "if \"Country\" in df.columns:\n",
    "    df_germany = df[df[\"Country\"].str.contains(\"Germany|Countries\", case=False, na=False)]\n",
    "\n",
    "\n",
    "columns = [\"Date\"]\n",
    "df_germany = df_germany[columns]\n",
    "df_germany[\"Date\"] = pd.to_datetime(df_germany[\"Date\"])\n",
    "\n",
    "# Filters for strikes only in the relevant timeframe\n",
    "df_germany = df_germany[(df_germany[\"Date\"] >= \"2019-01-01\") & (df_germany[\"Date\"] < \"2021-01-01\")]\n",
    "df_germany.to_csv(\"fff_germany_strikes.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e255a7",
   "metadata": {},
   "source": [
    "This part focuses on gathering and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8cb3a-e429-419e-81aa-b5f6fe60ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the raw data for 3 specific times\n",
    "x,y = search_video(\"2019-06-01T00:00:00Z\", \"2019-01-01T00:00:00Z\", filename=\"dataSetTilJune.csv\")\n",
    "x2,y2 = search_video(\"2020-01-01T00:00:00Z\", \"2019-06-01T00:00:00Z\", filename=\"dataSetTilDec20.csv\")\n",
    "x3,y3 = search_video(\"2021-01-01T00:00:00Z\", \"2020-01-01T00:00:00Z\", filename=\"dataSetTilDec21.csv\")\n",
    "\n",
    "# Processes the Data\n",
    "data1 = get_data(x)\n",
    "video_count(data1, \"data1_num.csv\", \"data1_sort.csv\")\n",
    "data2 = get_data(x2)\n",
    "video_count(data2, \"data2_num.csv\", \"data2_sort.csv\")\n",
    "data3 = get_data(x3)\n",
    "video_count(data3, \"data3_num.csv\", \"data3_sort.csv\")\n",
    "\n",
    "df1 = pd.read_csv(\"data1_num.csv\")\n",
    "df2 = pd.read_csv(\"data2_num.csv\")\n",
    "df3 = pd.read_csv(\"data3_num.csv\")\n",
    "\n",
    "# Combines the number of videos per day over all 3 periods\n",
    "df_all = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df_all.to_csv(\"combined_numbers.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35581318-0975-4d78-812c-74451823a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the keywords across all three time periods\n",
    "dfx = pd.read_csv(\"dataSetTilDec20.csv\")\n",
    "dfy = pd.read_csv(\"dataSetTilDec21.csv\")\n",
    "dfz = pd.read_csv(\"dataSetTilJune.csv\")\n",
    "full_data = pd.concat([dfx, dfy, dfz], ignore_index=True)\n",
    "keywords(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_csv(\"keywords.csv\")[\"words\"].str.lower().tolist()\n",
    "\n",
    "def check_keywords(video_df):\n",
    "    '''Checks which keywords appear in the top 15\n",
    "    most viewed videos\n",
    "    i: csv with videos sorted by view count\n",
    "    o: dict with keywords + amount'''\n",
    "    df = pd.read_csv(video_df)\n",
    "    df = df.head(15)\n",
    "    counts = Counter()\n",
    "    \n",
    "    # Iterates over each video\n",
    "    for _, row in df.iterrows():\n",
    "        title = str(row[\"title\"]).lower()\n",
    "        # Finds which keywords are present in the title\n",
    "        found = {kw for kw in keywords if kw in title}\n",
    "        # Increments count for each found keyowrd\n",
    "        for kw in found:\n",
    "            counts[kw] += 1\n",
    "    \n",
    "    return counts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds keywords in each dataset\n",
    "counts1 = check_keywords(\"data1_sort.csv\")\n",
    "counts2 = check_keywords(\"data2_sort.csv\")\n",
    "counts3 = check_keywords(\"data3_sort.csv\")\n",
    "pd.DataFrame.from_dict(counts1, orient=\"index\", columns=[\"count\"]).to_csv(\"keyword_counts_Jan2019.csv\")\n",
    "pd.DataFrame.from_dict(counts2, orient=\"index\", columns=[\"count\"]).to_csv(\"keyword_counts_June2019.csv\")\n",
    "pd.DataFrame.from_dict(counts3, orient=\"index\", columns=[\"count\"]).to_csv(\"keyword_counts_Jan2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce4ded",
   "metadata": {},
   "source": [
    "This part focuses on visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf512818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a wordcloud based on the keywords\n",
    "keywords_df = pd.read_csv(\"keywords.csv\")\n",
    "word_freq = dict(zip(keywords_df[\"words\"], keywords_df[\"numbers\"]))\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    width=2500,\n",
    "    height=1000,\n",
    "    colormap=\"winter\",\n",
    "    scale=2.0,\n",
    "    min_font_size=6\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "# Save to file\n",
    "wordcloud.to_file(\"word_cloud.png\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14dccb-2c0f-4933-a5d5-90f144060bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load youtube video protest dates \n",
    "df = pd.read_csv(\"combined_numbers.csv\", parse_dates=[\"year_month_day\"])\n",
    "df_protests = pd.read_csv(\"fff_germany_strikes.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Filters data for 2019\n",
    "df_2019 = df[df[\"year_month_day\"].dt.year == 2019]\n",
    "protests_2019 = df_protests[df_protests[\"Date\"].dt.year == 2019]\n",
    "\n",
    "# Plot video counts over time \n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(df_2019[\"year_month_day\"], df_2019[\"video_count\"], color=\"red\", label=\"Youtube Videos amount\",  path_effects=[path_effects.SimpleLineShadow(),\n",
    "                       path_effects.Normal()])\n",
    "\n",
    "# Overlay protest dates\n",
    "for protest_day in protests_2019[\"Date\"]:\n",
    "    plt.axvline(protest_day, color=\"blue\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "\n",
    "# Displays number of videos per day and protest days for 2019\n",
    "plt.plot([], [], color=\"blue\", linestyle=\"--\", alpha=0.5, label=\"Protest day\")\n",
    "plt.title(\"YouTube Videos about Fridays for Future (2019)\", fontsize=15)\n",
    "plt.xlim(df[\"year_month_day\"].min(),pd.Timestamp(\"2020-01-01\"))\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylabel(\"Number of videos\", fontsize=15)\n",
    "plt.xlabel(\"Date\", fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.savefig(\"graph.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Filters data for 2020\n",
    "df_2020 = df[df[\"year_month_day\"].dt.year == 2020]\n",
    "protests_2020 = df_protests[df_protests[\"Date\"].dt.year == 2020]\n",
    "\n",
    "# Plot video counts over time \n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(df_2020[\"year_month_day\"], df_2020[\"video_count\"], color=\"red\", label=\"Youtube Videos amount\", path_effects=[path_effects.SimpleLineShadow(),\n",
    "                       path_effects.Normal()])\n",
    "\n",
    "# Overlay protest dates\n",
    "for protest_day in protests_2020[\"Date\"]:\n",
    "    plt.axvline(protest_day, color=\"blue\", linestyle=\"--\", alpha=0.5, label=\"Protest day\")\n",
    "\n",
    "# Displays number of videos per day and protest days for 2020\n",
    "plt.title(\"YouTube Videos about Fridays for Future (2020)\", fontsize=15)\n",
    "plt.xlim(pd.Timestamp(\"2020-01-01\"),df[\"year_month_day\"].max())\n",
    "plt.ylabel(\"Number of videos\", fontsize=15)\n",
    "plt.xlabel(\"Date\", fontsize=15)\n",
    "plt.yticks( fontsize=12)\n",
    "plt.xticks( fontsize=12)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68884d87-1d6b-4690-adb5-292562dacf46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combines all keywords from the datasets into a single list\n",
    "all_keywords = list(set(counts1.keys()) | set(counts2.keys()) | set(counts3.keys()))\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"Jan2019 - May2019\": [counts1.get(k,0) for k in all_keywords],\n",
    "    \"June2019 - Dec2019\": [counts2.get(k,0) for k in all_keywords],\n",
    "    \"Jan2020 - Dec2020\": [counts3.get(k,0) for k in all_keywords]\n",
    "}, index=all_keywords)\n",
    "\n",
    "# Handles \"protest\" and \"proteste\" being both present in the list\n",
    "if \"proteste\" in df_counts.index and \"protest\" in df_counts.index:\n",
    "    df_counts.loc[\"protest\"] = df_counts.loc[[\"protest\", \"proteste\"]].sum()\n",
    "    df_counts = df_counts.drop(\"proteste\")\n",
    "\n",
    "# Adds a total sum occurance to filter out keywords that appeared only once or twice\n",
    "df_counts[\"total\"] = df_counts.sum(axis=1)\n",
    "df_filtered = df_counts[df_counts[\"total\"] > 2].drop(columns=\"total\")\n",
    "df_filtered.to_csv(\"keyword_counts_combined.csv\", encoding=\"utf-8\")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_filtered, annot=True, fmt=\"d\", cmap=\"BuPu\")\n",
    "plt.title(\"Keyword occurences in Top 15 most viewed videos\")\n",
    "plt.ylabel(\"Keyword\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c0bc5-3773-43e0-b388-aecfa0ead6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
